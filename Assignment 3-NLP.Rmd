---
title: "Assignment 3 - NLP"
author: "Charles Lang"
date: "2/23/2017"
output: html_document
---

## Libraries
```{r}
#Make sure you install and load the following libraries

library(tm)
library(SnowballC)
library(wordcloud)
library(ggplot2)
library(dplyr)
library(tidyr)
library(topicmodels)

#IF USING A MAC PLEASE RUN THIS CODE
Sys.setlocale("LC_ALL", "C")
```

## Import all document files and the list of weeks file
```{r}
#Create a list of all the files
file.list <- list.files(path="~/YOUR FILE PATH", pattern=".csv")
#Loop over file list importing them and binding them together
D1 <- do.call("rbind", lapply(file.list, read.csv, header = TRUE, stringsAsFactors = FALSE))

D2 <- read.csv("~/YOUR FILE PATH/week-list.csv", header = TRUE)
```

## Clean the htlm tags from your text
```{r}

```

## Merge with week list so you have a variable representing weeks for each entry 
```{r}

```

## Process text using the tm package - Code has been altered to account for changes in the tm package
```{r}
#Convert the data frame to the corpus format that the tm package uses
corpus <- Corpus(VectorSource(D1$Notes2))
#Remove spaces
corpus <- tm_map(corpus, stripWhitespace)
#Convert to lower case
corpus <- tm_map(corpus, content_transformer(tolower)) 
#Remove pre-defined stop words ('the', 'a', etc)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
#Convert words to stems ("education" = "edu") for analysis, for more info see  http://tartarus.org/~martin/PorterStemmer/
corpus <- tm_map(corpus, stemDocument, lazy=TRUE)
#Remove numbers
corpus <- tm_map(corpus, removeNumbers, lazy=TRUE)
#remove punctuation
corpus <- tm_map(corpus, removePunctuation, lazy=TRUE)
```

#### Create a Term Document Matrix
```{r}
#Convert corpus to a term document matrix - so each word can be analyzed individuallly
tdm.corpus <- TermDocumentMatrix(corpus)
```

# Sentiment Analysis

### Match words in corpus to lexicons of positive & negative words
```{r}
#Upload positive and negative word lexicons
positive <- readLines("positive-words.txt")
negative <- readLines("negative-words.txt")

#Search for matches between each word and the two lexicons
D1$positive <- tm_term_score(tdm.corpus, positive)
D1$negative <- tm_term_score(tdm.corpus, negative)

#Generate an overall pos-neg score for each line
D1$score <- D1$positive - D1$negative

```

## Generate a visualization of the sum of the sentiment score over weeks
```{r}

```

# LDA Topic Modelling

Using the same csv file you have generated the LDA analysis will treat each row of the data frame as a document. Does this make sense for generating topics?

```{r}
#Term Frequency Inverse Document Frequency
dtm.tfi <- DocumentTermMatrix(corpus, control = list(weighting = weightTf))

#Remove very uncommon terms (term freq inverse document freq < 0.1)
dtm.tfi <- dtm.tfi[,dtm.tfi$v >= 0.1]

#Remove non-zero entries
rowTotals <- apply(dtm.tfi , 1, sum) #Find the sum of words in each Document
dtm.tfi   <- dtm.tfi[rowTotals> 0, ] #Divide by sum across rows

lda.model = LDA(dtm.tfi, k = 3, seed = 150)

#Which terms are most common in each topic
terms(lda.model)

#Which documents belong to which topic
topics(lda.model)

```

What does an LDA topic represent? 

# Main Task 

Your task is to generate a *single* visualization showing: 

- Sentiment for each week and 
- One important topic for that week

